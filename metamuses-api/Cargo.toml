[package]
name = "metamuses-api"
version = "0.1.0"
edition = "2021"

[dependencies]
# Alith AI Framework
# Note: Using default-features = false to avoid llama-cpp-2 which has ARM64 build issues
# The llama-cpp-2 crate has c_char type mismatch on ARM64 Linux (u8 vs i8)
alith = { git = "https://github.com/0xLazAI/alith", default-features = false, features = ["ipfs", "fastembed", "qdrant"] }

# llama.cpp Rust bindings - significantly faster than Candle for inference
# Platform-specific features are configured in target sections below
llama_cpp = "0.3"

# Keep Candle as fallback (optional, can be removed if not needed)
candle-core = { version = "0.9", optional = true }
candle-nn = { version = "0.9", optional = true }
candle-transformers = { version = "0.9", optional = true }

# Tokenizers for text processing
tokenizers = "0.21"
hf-hub = "0.3"

# Thread pool for parallel computation
rayon = "1.10"
num_cpus = "1.16"

# Async Runtime
tokio = { version = "1.47", features = ["full"] }
async-trait = "0.1"
futures = "0.3"

# Web Framework
axum = { version = "0.8", features = ["multipart", "ws"] }
tower = "0.5"
tower-http = { version = "0.6", features = ["cors", "trace"] }

# HTTP Client for External APIs
reqwest = { version = "0.12", features = ["json", "rustls-tls"] }

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Database
sqlx = { version = "0.8", features = ["runtime-tokio", "postgres", "uuid", "chrono", "migrate"] }

# Redis
redis = { version = "0.27", features = ["tokio-comp", "connection-manager"] }

# Vector Database & Embeddings
qdrant-client = "1.13"
fastembed = "4.9"

# Error Handling
anyhow = "1.0"
thiserror = "2.0"

# Logging & Metrics
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
prometheus = "0.13"

# Utilities
uuid = { version = "1.17", features = ["v4", "serde"] }
chrono = { version = "0.4", features = ["serde"] }
dotenv = "0.15"
lazy_static = "1.5"
rand = "0.8"
regex = "1.11"

# Existing MetaMuse dependencies
ethers = { version = "2.0", features = ["abigen", "rustls"] }
sha256 = "1.0"
sha3 = "0.10"
hex = "0.4"

[dev-dependencies]
criterion = "0.5"
mockall = "0.13"

[[bin]]
name = "metamuse-server"
path = "src/bin/server.rs"

[[bin]]
name = "metamuse-worker"
path = "src/bin/worker.rs"

[[bin]]
name = "test-inference"
path = "src/bin/test_inference.rs"

[features]
default = ["llama-cpp"]
llama-cpp = []
candle = ["candle-core", "candle-nn", "candle-transformers"]

# Platform-specific dependencies (must be at the end)
[target.'cfg(target_os = "macos")'.dependencies]
# Metal GPU acceleration for Apple Silicon
llama_cpp = { version = "0.3", features = ["metal"] }

[target.'cfg(target_os = "linux")'.dependencies]
# CPU-only for Linux (works on ARM64 and x86_64)
llama_cpp = { version = "0.3" }
