// Simple test binary to verify model loading and inference
// Usage: cd metamuses-api && cargo run --bin test-inference

use anyhow::Result;
use metamuses_api::{
    config::Config,
    inference::{models, ModelFactory},
    types::ModelTier,
};

#[tokio::main]
async fn main() -> Result<()> {
    // Initialize logging
    tracing_subscriber::fmt()
        .with_target(false)
        .with_level(true)
        .init();

    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘  MetaMuses Model Inference Test       â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // Load configuration
    println!("ğŸ“ Loading configuration...");
    dotenv::dotenv().ok();
    let config = Config::from_env()?;
    println!("âœ… Configuration loaded");
    println!("   Models directory: {}\n", config.models_dir);

    // Check models directory
    println!("ğŸ“‚ Checking models directory...");
    let models_dir = models::get_models_dir();
    println!("   Resolved path: {}", models_dir);

    let model_path =
        std::path::PathBuf::from(&models_dir).join("Qwen3-4B-Instruct-2507-IQ4_XS.gguf");
    if model_path.exists() {
        println!("âœ… Model file found: {}", model_path.display());

        // Get file size
        if let Ok(metadata) = std::fs::metadata(&model_path) {
            let size_mb = metadata.len() as f64 / (1024.0 * 1024.0);
            println!("   Size: {:.2} MB", size_mb);
        }
    } else {
        println!("âŒ Model file not found: {}", model_path.display());
        println!("   Please ensure the model file is in the correct location.");
        return Ok(());
    }
    println!();

    // Test model registry
    println!("ğŸ” Testing model registry...");
    let registry = models::get_model_registry();
    println!("   Available tiers: {}", registry.len());

    for (tier, configs) in &registry {
        println!("   - {:?}: {} models", tier, configs.len());
        for config in configs {
            println!("     â€¢ {} ({})", config.model_name, config.model_path);
        }
    }
    println!();

    // Test loading Fast tier model
    println!("ğŸš€ Testing Fast tier model loading...");
    match models::get_model_config(ModelTier::Fast) {
        Some(model_config) => {
            println!("âœ… Model config found:");
            println!("   Name: {}", model_config.model_name);
            println!("   Path: {}", model_config.model_path);
            println!("   Context: {} tokens", model_config.context_length);
            println!("   Threads: {:?}", model_config.num_threads);
            println!();

            // Try to create engine
            println!("ğŸ”§ Creating inference engine...");
            match ModelFactory::create_engine(&model_config).await {
                Ok(engine) => {
                    println!("âœ… Engine created successfully");
                    println!("   Model: {}", engine.model_name());
                    println!("   Tier: {:?}", engine.tier());
                    println!();

                    // Test inference
                    println!("ğŸ’¬ Testing inference...");
                    let test_prompts = vec![
                        "Hello! How are you today?",
                        "What is 2+2?",
                        "Write a haiku about AI.",
                    ];

                    for (i, prompt) in test_prompts.iter().enumerate() {
                        println!("\nğŸ“¨ Prompt {}: {}", i + 1, prompt);

                        let start = std::time::Instant::now();
                        match engine.generate(prompt).await {
                            Ok(response) => {
                                let duration = start.elapsed();
                                println!("âœ… Response ({}ms):", duration.as_millis());
                                println!("   {}", response.trim());
                            }
                            Err(e) => {
                                println!("âŒ Error: {}", e);
                            }
                        }
                    }
                    println!();
                }
                Err(e) => {
                    println!("âŒ Failed to create engine: {}", e);
                    println!("   This could be due to:");
                    println!("   - Model file corrupted or incompatible");
                    println!("   - Insufficient memory");
                    println!("   - Missing dependencies (llama.cpp)");
                }
            }
        }
        None => {
            println!("âŒ No model config found for Fast tier");
        }
    }

    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘  Test Complete                        â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    Ok(())
}
